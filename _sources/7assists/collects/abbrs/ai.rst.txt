AIç›¸å…³åè¯
##########


* Linear Algebra[ËˆÃ¦ldÊ’ÉªbrÉ™]: çº¿æ€§ä»£æ•°
* Probability Statistics: æ¦‚ç‡ç»Ÿè®¡
* probability: æ¦‚ç‡è®º
* Calculus: å¾®ç§¯åˆ†å­¦
* deviation: åå·®
* Covariance: åæ–¹å·®
* Entropy: ç†µ
* Multivariable Calculus: å¤šå…ƒå¾®ç§¯åˆ†
* Standard deviation: æ ‡å‡†åå·®
* squared error: å¹³æ–¹è¯¯å·®
* differentiability: å¯å¾®æ€§


* scalar: n. [æ•°] æ ‡é‡, adj. æ ‡é‡çš„ï¼›æ•°é‡çš„ï¼›æ¢¯çŠ¶çš„ï¼Œåˆ†ç­‰çº§çš„
* vector: n. [æ•°] å‘é‡ï¼›çŸ¢é‡ï¼›å¸¦èŒè€…ï¼›èˆªçº¿, vt. ç”¨æ— çº¿ç”µå¯¼èˆª
* matrix: n. [æ•°] çŸ©é˜µï¼›æ¨¡å‹ï¼›ç¤¾ä¼šç¯å¢ƒï¼›åŸºè´¨ï¼›æ¯ä½“ï¼›å­å®«ï¼›è„‰çŸ³
* tensor: n. [æ•°] å¼ é‡ï¼›[è§£å‰–] å¼ è‚Œ
* logarithm: n. [æ•°] å¯¹æ•°
* exponential: n. [æ•°] æŒ‡æ•°
* derivative:  n. [æ•°] å¯¼æ•° [åŒ–å­¦] è¡ç”Ÿç‰©ï¼›é‡‘èè¡ç”Ÿäº§å“ï¼›æ´¾ç”Ÿè¯ï¼›
* Partial derivative: åå¯¼æ•°
* Gradient: n. [æ•°] æ¢¯åº¦ï¼›å¡åº¦ï¼›å€¾æ–œåº¦


* Hadamard: é˜¿è¾¾ç›ï¼ˆHadamardï¼‰çŸ©é˜µï¼šç®€ç§°HçŸ©é˜µ. Â®HçŸ©é˜µæ˜¯ä¸€ä¸ªæ–¹é˜µï¼Œä»…æœ‰å…ƒç´ +1å’Œ-1æ„æˆï¼Œè€Œä¸”å…¶å„è¡Œï¼ˆå’Œåˆ—ï¼‰ æ˜¯äº’ç›¸æ­£äº¤çš„

* identity matrix: n. [æ•°] å•ä½çŸ©é˜µ
* the indicator function: æŒ‡æ ‡å‡½æ•°
* transpose of a vector or a matrix: ä¸€ä¸ªå‘é‡æˆ–çŸ©é˜µçš„è½¬ç½®
* Inverse of matrix X: çŸ©é˜µXçš„é€†çŸ©é˜µ
* Gradient of  ğ‘¦  with respect to  ğ±: yå…³äºxçš„æ¢¯åº¦
* Definite integral of  ğ‘“  from  ğ‘  to  ğ‘  with respect to  ğ‘¥: få…³äºxä»aåˆ°bçš„å®šç§¯åˆ†
* Random variable ğ‘§ has probability distribution ğ‘ƒ: éšæœºå˜é‡zçš„æ¦‚ç‡åˆ†å¸ƒä¸ºp

* loss function: æŸå¤±å‡½æ•°
* clustering algorithm: èšç±»ç®—æ³•
* Standard deviation: æ ‡å‡†åå·®
* normal equation: æ­£è§„æ–¹ç¨‹


* Probability distribution: æ¦‚ç‡åˆ†å¸ƒ
* Bernoulli distribution: ä¼¯åŠªåˆ©åˆ†å¸ƒ
* Gaussian distribution: é«˜æ–¯åˆ†å¸ƒ
* Monte Carlo tree: è’™ç‰¹å¡ç½—æ ‘
* Conditional probability: æ¡ä»¶æ¦‚ç‡
* Probability density function: [æ•°] æœºç‡å¯†åº¦å‡½æ•°ï¼›æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå‡½æ•°



* linear regression: çº¿æ€§å›å½’
* logistic regression: é€»è¾‘å›å½’



* stochastic gradient descent: éšæœºæ¢¯åº¦ä¸‹é™
* multi-layer perceptrons: å¤šå±‚æ„ŸçŸ¥å™¨

* pixel: åƒç´ ,åˆ†è¾¨ç‡

* the  ğ¿2  loss corresponds to the assumption that our data was corrupted by Gaussian noise, whereas the  ğ¿1  loss corresponds to an assumption of noise from a Laplace distribution.

* deep learning: æ·±åº¦å­¦ä¹ 
* deep reinforcement learning: æ·±åº¦å¼ºåŒ–å­¦ä¹ 





